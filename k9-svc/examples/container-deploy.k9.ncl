# SPDX-License-Identifier: PMPL-1.0-or-later
# container-deploy.k9.ncl - Multi-service container deployment component
#
# Copyright (c) 2026 Jonathan D.A. Jewell (hyperpolymath)
#
# Security Level: 'Hunt (requires cryptographic handshake)
#
# This is a realistic container deployment component that demonstrates
# how k9-svc wraps selur-compose deployments with typed configuration,
# security-level enforcement, rolling deployment, and rollback.
#
# It deploys a three-service stack (app, database, cache) across
# dev/staging/prod environments using selur-compose (Podman-first).
#
# WARNING: This is a Hunt-level component. It can execute shell commands,
# modify the filesystem, and access the network. It REQUIRES a valid
# Ed25519 signature before any execution is permitted.
#
# Usage:
#   just authorize examples/container-deploy.k9.ncl
#   just deploy-stack dev
#   just deploy-stack staging
#   just deploy-stack production

let pedigree = import "../pedigree.ncl" in
let leash = import "../leash.ncl" in

# ─────────────────────────────────────────────────────────────────────
# Component Pedigree (L1-L5)
#
# This is the self-description that every k9 component carries.
# The pedigree tells the runtime WHO this component is, WHERE it
# runs, WHAT it's allowed to do, and HOW it validates itself.
# ─────────────────────────────────────────────────────────────────────
let component_pedigree = {
  metadata = {
    name = "container-deploy-stack",
    version = "1.0.0",
    breed = "application/vnd.k9+nickel",
    magic_number = "K9!",
    description = "Multi-service container deployment with rolling updates and rollback",
  },

  # Target: Linux with Podman, needs at least 1GB for the full stack.
  # is_edge = false because this is a server-side deployment component,
  # not something that runs on constrained hardware.
  target = {
    os = 'Linux,
    is_edge = false,
    requires_podman = true,
    min_memory_mb = 1024,
  },

  # Security: Hunt level because this component:
  # - Runs selur-compose commands (subprocess)
  # - Pulls container images (network)
  # - Writes deployment state files (filesystem)
  # All three I/O capabilities are needed, so Hunt is the correct level.
  # Kennel and Yard cannot perform any of these operations.
  security = {
    trust_level = 'Hunt,
    allow_network = true,
    allow_filesystem_write = true,
    allow_subprocess = true,
    # In production, this MUST be a real Ed25519 signature generated by:
    #   just sign examples/container-deploy.k9.ncl <key-name>
    # The placeholder below will cause the Leash system to reject execution
    # until a real signature is provided.
    signature = "PLACEHOLDER-REQUIRES-REAL-ED25519-SIGNATURE",
  },

  # Validation: The checksum is computed over the component payload
  # (everything except the signature field itself). The pedigree_version
  # tracks which version of the pedigree schema this component was
  # written against. hunt_authorized starts false and is set true only
  # after the Leash handshake completes successfully.
  validation = {
    checksum = "sha256:placeholder-recomputed-at-sign-time",
    pedigree_version = "1.0.0",
    hunt_authorized = false,
  },

  # Recipes: The Just targets that implement each lifecycle phase.
  # These are the actual commands the k9 runtime will invoke.
  recipes = {
    install = "just install-stack",
    validate = "just validate-stack",
    deploy = "just deploy-stack",
    migrate = "just migrate-stack",
  },
} in

# ─────────────────────────────────────────────────────────────────────
# Service Definitions
#
# Each service in the stack is defined as a typed Nickel record.
# Nickel contracts enforce that port numbers are valid, memory
# limits are reasonable, and required fields are present.
#
# This is where k9-svc shines compared to raw docker-compose.yml:
# invalid configurations are caught at evaluation time, BEFORE
# any container is started.
# ─────────────────────────────────────────────────────────────────────

# Contract: A valid TCP/UDP port number (1-65535)
let ValidPort = std.contract.from_predicate (fun p => p > 0 && p < 65536) in

# Contract: A valid replica count (at least 1, at most 10 for safety)
let ValidReplicas = std.contract.from_predicate (fun r => r >= 1 && r <= 10) in

let services = {
  # The primary application service.
  # Uses Chainguard base images for minimal attack surface.
  app = {
    image = "ghcr.io/hyperpolymath/myapp",
    port | ValidPort = 8080,
    health_check = "/api/health",
    readiness_check = "/api/ready",
    # Liveness probe: if the app doesn't respond within 5 seconds,
    # Podman will restart it. This prevents zombie processes.
    liveness_interval_seconds = 10,
    liveness_timeout_seconds = 5,
    depends_on = ["database", "cache"],
  },

  # PostgreSQL database service.
  # Chainguard provides a hardened PostgreSQL image with no shell,
  # no package manager, and minimal CVE surface.
  database = {
    image = "cgr.dev/chainguard/postgres:latest",
    port | ValidPort = 5432,
    health_check = "pg_isready -U app -d myapp",
    # Persistent volume for data. Without this, database state
    # is lost on container restart.
    volume = "pgdata:/var/lib/postgresql/data",
    depends_on = [],
  },

  # Redis-compatible cache (Valkey).
  # Valkey is the community fork of Redis, fully compatible.
  # We use it for session caching and rate limiting.
  cache = {
    image = "cgr.dev/chainguard/valkey:latest",
    port | ValidPort = 6379,
    health_check = "valkey-cli ping",
    depends_on = [],
  },
} in

# ─────────────────────────────────────────────────────────────────────
# Environment-Specific Configuration
#
# Each environment overrides the base service definitions with
# resource limits, replica counts, and image tags appropriate
# for that deployment tier.
#
# The Nickel type system ensures that dev/staging/prod configs
# all share the same shape, so you cannot accidentally omit a
# field in production that exists in dev.
# ─────────────────────────────────────────────────────────────────────
let environments = {
  # Development: Single replica, generous resources, debug logging.
  # Uses :dev image tags built from the current branch.
  dev = {
    replicas | ValidReplicas = 1,
    app_memory = "256Mi",
    app_cpu = "100m",
    db_memory = "256Mi",
    cache_memory = "64Mi",
    image_tag = "dev",
    log_level = "debug",
    # In dev, we allow insecure connections for local testing.
    # This flag is checked by the deploy script and NEVER set in prod.
    allow_insecure = true,
    # Domain for the app service (used in health check URLs)
    domain = "localhost",
  },

  # Staging: Mirrors production topology but with fewer resources.
  # Uses :staging tags from the release branch.
  staging = {
    replicas | ValidReplicas = 2,
    app_memory = "512Mi",
    app_cpu = "250m",
    db_memory = "512Mi",
    cache_memory = "128Mi",
    image_tag = "staging",
    log_level = "info",
    allow_insecure = false,
    domain = "staging.example.com",
  },

  # Production: Full resources, multiple replicas, strict security.
  # Uses :latest tags from the main branch (tagged releases only).
  production = {
    replicas | ValidReplicas = 3,
    app_memory = "1Gi",
    app_cpu = "500m",
    db_memory = "2Gi",
    cache_memory = "256Mi",
    image_tag = "latest",
    log_level = "warn",
    allow_insecure = false,
    domain = "app.example.com",
  },
} in

# ─────────────────────────────────────────────────────────────────────
# Deployment Strategy
#
# Rolling deployment: replace one replica at a time, wait for health
# checks to pass before proceeding to the next. If any replica fails
# health checks, halt the rollout and trigger rollback.
#
# max_surge = 1 means we can temporarily have one extra replica
# during the transition. max_unavailable = 0 means we never reduce
# capacity below the target count — zero-downtime deployment.
# ─────────────────────────────────────────────────────────────────────
let strategy = {
  type = "rolling",
  max_surge = 1,
  max_unavailable = 0,
  # How many seconds to wait after a new replica starts before
  # checking its health. Gives the app time to initialize.
  startup_delay_seconds = 10,
  # How many consecutive health check passes are required before
  # declaring a replica healthy.
  health_check_threshold = 3,
  # Maximum time (seconds) to wait for a replica to become healthy.
  # If exceeded, the rollout is halted and rollback begins.
  health_check_timeout_seconds = 120,
  # Whether to automatically rollback on failure.
  # If false, the deployment halts but does not revert.
  auto_rollback = true,
} in

# ─────────────────────────────────────────────────────────────────────
# Certificate and Signing Verification
#
# Before deploying to staging or production, the component verifies:
# 1. The container images are signed (cerro-torre .ctp bundles)
# 2. The SBOM (Software Bill of Materials) is present and valid
# 3. The k9 component itself has a valid Ed25519 signature
#
# This is the supply chain security layer. Without it, you're
# trusting that the image you pulled is the image you built.
# ─────────────────────────────────────────────────────────────────────
let signing = {
  # Whether to verify container image signatures before deployment.
  # Always true in staging/production. Can be false in dev for speed.
  verify_images = true,
  # The cerro-torre key ID used to sign container images.
  # This must match the key in your organization's trust store.
  image_signing_key_id = "hyperpolymath-release-2026",
  # Whether to require SBOM attestation on images.
  require_sbom = true,
  # Accepted SBOM formats (SPDX or CycloneDX)
  sbom_formats = ["spdx", "cyclonedx"],
  # Whether to verify this k9 component's own signature.
  # Always true — this is enforced by the Leash system anyway.
  verify_self = true,
} in

# ─────────────────────────────────────────────────────────────────────
# Deployment Scripts
#
# These are the actual shell scripts executed at Hunt level.
# Each script is a self-contained POSIX shell program embedded
# in the Nickel record. The k9 runtime extracts and executes them.
#
# SECURITY NOTE: These scripts have full system access. Review
# every line before authorizing this component.
# ─────────────────────────────────────────────────────────────────────
let scripts = {
  # ── Pre-deployment validation ──────────────────────────────────
  # Runs before any containers are touched. Checks:
  # - Nickel contracts pass (typed config is valid)
  # - Podman/selur-compose are available
  # - Target environment has enough resources
  # - Container images exist and are signed
  pre_deploy = m%"
#!/bin/sh
# SPDX-License-Identifier: PMPL-1.0-or-later
# Pre-deployment validation for container-deploy-stack
set -eu

ENV="${1:-dev}"
echo "K9: Pre-deployment validation for environment: $ENV"

# Step 1: Validate the k9 component itself (Nickel typecheck)
echo "K9: [1/5] Typechecking component..."
nickel typecheck examples/container-deploy.k9.ncl
echo "K9: Typecheck passed."

# Step 2: Verify Podman and selur-compose are available
echo "K9: [2/5] Checking runtime dependencies..."
command -v podman >/dev/null 2>&1 || { echo "ERROR: podman not found"; exit 1; }
command -v selur-compose >/dev/null 2>&1 || { echo "ERROR: selur-compose not found"; exit 1; }
echo "K9: Runtime dependencies present."

# Step 3: Check available system resources
echo "K9: [3/5] Checking system resources..."
AVAILABLE_MB=$(free -m | awk '/^Mem:/ {print $7}')
if [ "$AVAILABLE_MB" -lt 1024 ]; then
    echo "WARNING: Less than 1GB available memory ($AVAILABLE_MB MB)"
    if [ "$ENV" = "production" ]; then
        echo "ERROR: Insufficient memory for production deployment"
        exit 1
    fi
fi
echo "K9: System resources adequate ($AVAILABLE_MB MB available)."

# Step 4: Verify container image signatures (staging/production only)
if [ "$ENV" != "dev" ]; then
    echo "K9: [4/5] Verifying container image signatures..."
    for IMAGE in ghcr.io/hyperpolymath/myapp cgr.dev/chainguard/postgres cgr.dev/chainguard/valkey; do
        echo "K9: Checking signature for $IMAGE..."
        # cerro-torre verify checks the .ctp signature bundle
        cerro-torre verify "$IMAGE" --key-id hyperpolymath-release-2026 || {
            echo "ERROR: Image signature verification failed for $IMAGE"
            exit 1
        }
    done
    echo "K9: All image signatures verified."
else
    echo "K9: [4/5] Skipping image signature verification (dev environment)."
fi

# Step 5: Verify this component's own Ed25519 signature
echo "K9: [5/5] Verifying component signature..."
./must verify examples/container-deploy.k9.ncl || {
    echo "ERROR: Component signature verification failed"
    exit 1
}
echo "K9: Component signature verified."

echo "K9: Pre-deployment validation PASSED for $ENV."
"%,

  # ── Generate selur-compose configuration ────────────────────────
  # Produces a selur-compose.toml from the Nickel configuration.
  # This is where the typed config becomes a concrete deployment
  # manifest. The Nickel evaluation has already validated all values,
  # so the generated TOML is guaranteed to be well-formed.
  generate_compose = m%"
#!/bin/sh
# SPDX-License-Identifier: PMPL-1.0-or-later
# Generate selur-compose.toml from k9 configuration
set -eu

ENV="${1:-dev}"
OUTFILE="selur-compose.${ENV}.toml"

echo "K9: Generating $OUTFILE..."

# Extract environment-specific config via Nickel
# nickel export outputs the evaluated config as JSON,
# which we then transform into selur-compose TOML format.
nickel export --format json examples/container-deploy.k9.ncl \
  | jq -r --arg env "$ENV" '.environments[$env]' \
  > "/tmp/k9-env-config.json"

TAG=$(jq -r '.image_tag' /tmp/k9-env-config.json)
REPLICAS=$(jq -r '.replicas' /tmp/k9-env-config.json)
APP_MEM=$(jq -r '.app_memory' /tmp/k9-env-config.json)
DB_MEM=$(jq -r '.db_memory' /tmp/k9-env-config.json)
CACHE_MEM=$(jq -r '.cache_memory' /tmp/k9-env-config.json)
LOG_LEVEL=$(jq -r '.log_level' /tmp/k9-env-config.json)
DOMAIN=$(jq -r '.domain' /tmp/k9-env-config.json)

cat > "$OUTFILE" << TOML
# SPDX-License-Identifier: PMPL-1.0-or-later
# Auto-generated by container-deploy-stack k9 component
# Environment: $ENV
# Generated: $(date -u +%Y-%m-%dT%H:%M:%SZ)
# DO NOT EDIT — regenerate with: just generate-compose $ENV

[services.app]
image = "ghcr.io/hyperpolymath/myapp:$TAG"
ports = ["8080:8080"]
replicas = $REPLICAS
memory_limit = "$APP_MEM"
depends_on = ["database", "cache"]
healthcheck_cmd = "wget -qO- http://localhost:8080/api/health || exit 1"
healthcheck_interval = "10s"
healthcheck_timeout = "5s"
healthcheck_retries = 3

[services.app.environment]
LOG_LEVEL = "$LOG_LEVEL"
DATABASE_URL = "postgresql://app:app@database:5432/myapp"
CACHE_URL = "valkey://cache:6379"
APP_DOMAIN = "$DOMAIN"

[services.database]
image = "cgr.dev/chainguard/postgres:latest"
ports = ["5432:5432"]
memory_limit = "$DB_MEM"
volumes = ["pgdata:/var/lib/postgresql/data"]
healthcheck_cmd = "pg_isready -U app -d myapp"
healthcheck_interval = "5s"
healthcheck_timeout = "3s"

[services.database.environment]
POSTGRES_USER = "app"
POSTGRES_DB = "myapp"

[services.cache]
image = "cgr.dev/chainguard/valkey:latest"
ports = ["6379:6379"]
memory_limit = "$CACHE_MEM"
healthcheck_cmd = "valkey-cli ping"
healthcheck_interval = "5s"
healthcheck_timeout = "3s"

[volumes]
pgdata = {}
TOML

echo "K9: Generated $OUTFILE successfully."
"%,

  # ── Rolling deployment with health checks ──────────────────────
  # The main deployment script. Performs:
  # 1. Pre-deployment validation
  # 2. Compose file generation
  # 3. Image pull (with signature verification)
  # 4. Rolling service update
  # 5. Post-deployment health check
  # 6. Rollback on failure
  deploy = m%"
#!/bin/sh
# SPDX-License-Identifier: PMPL-1.0-or-later
# Rolling deployment for container-deploy-stack
set -eu

ENV="${1:-dev}"
COMPOSE_FILE="selur-compose.${ENV}.toml"
STATE_FILE="/tmp/k9-deploy-state-${ENV}.json"
PREV_STATE_FILE="/tmp/k9-deploy-state-${ENV}.prev.json"

echo "=============================================="
echo "K9: Container Stack Deployment"
echo "K9: Environment: $ENV"
echo "K9: Timestamp:   $(date -u +%Y-%m-%dT%H:%M:%SZ)"
echo "=============================================="

# Save current state for rollback
if [ -f "$STATE_FILE" ]; then
    cp "$STATE_FILE" "$PREV_STATE_FILE"
    echo "K9: Previous deployment state saved for rollback."
fi

# Step 1: Pull images
echo ""
echo "K9: [1/4] Pulling container images..."
selur-compose -f "$COMPOSE_FILE" pull
echo "K9: Images pulled."

# Step 2: Start infrastructure services first (database, cache)
echo ""
echo "K9: [2/4] Starting infrastructure services..."
selur-compose -f "$COMPOSE_FILE" up -d database cache
echo "K9: Waiting for infrastructure health checks..."

# Wait for database
RETRIES=0
MAX_RETRIES=30
while [ $RETRIES -lt $MAX_RETRIES ]; do
    if selur-compose -f "$COMPOSE_FILE" exec database pg_isready -U app -d myapp >/dev/null 2>&1; then
        echo "K9: Database is ready."
        break
    fi
    RETRIES=$((RETRIES + 1))
    echo "K9: Waiting for database... ($RETRIES/$MAX_RETRIES)"
    sleep 2
done
if [ $RETRIES -eq $MAX_RETRIES ]; then
    echo "ERROR: Database failed to become ready within timeout."
    exit 1
fi

# Wait for cache
RETRIES=0
while [ $RETRIES -lt $MAX_RETRIES ]; do
    if selur-compose -f "$COMPOSE_FILE" exec cache valkey-cli ping >/dev/null 2>&1; then
        echo "K9: Cache is ready."
        break
    fi
    RETRIES=$((RETRIES + 1))
    echo "K9: Waiting for cache... ($RETRIES/$MAX_RETRIES)"
    sleep 2
done
if [ $RETRIES -eq $MAX_RETRIES ]; then
    echo "ERROR: Cache failed to become ready within timeout."
    exit 1
fi

# Step 3: Rolling update of app service
echo ""
echo "K9: [3/4] Rolling update of app service..."
selur-compose -f "$COMPOSE_FILE" up -d --no-deps app
echo "K9: App containers starting..."

# Step 4: Post-deployment health check
echo ""
echo "K9: [4/4] Running post-deployment health checks..."
HEALTH_PASSES=0
REQUIRED_PASSES=3
RETRIES=0
MAX_RETRIES=60
while [ $HEALTH_PASSES -lt $REQUIRED_PASSES ] && [ $RETRIES -lt $MAX_RETRIES ]; do
    if wget -qO- "http://localhost:8080/api/health" >/dev/null 2>&1; then
        HEALTH_PASSES=$((HEALTH_PASSES + 1))
        echo "K9: Health check passed ($HEALTH_PASSES/$REQUIRED_PASSES)"
    else
        HEALTH_PASSES=0
        echo "K9: Health check failed, resetting counter..."
    fi
    RETRIES=$((RETRIES + 1))
    sleep 2
done

if [ $HEALTH_PASSES -lt $REQUIRED_PASSES ]; then
    echo ""
    echo "ERROR: Deployment health checks failed after $MAX_RETRIES attempts."
    echo "K9: Initiating rollback..."
    if [ -f "$PREV_STATE_FILE" ]; then
        # Rollback to previous state
        selur-compose -f "$COMPOSE_FILE" down
        selur-compose -f "$COMPOSE_FILE" up -d
        echo "K9: Rollback complete. Previous deployment restored."
    else
        echo "WARNING: No previous state available for rollback."
        echo "K9: Services are in a potentially broken state."
    fi
    exit 1
fi

# Record deployment state
cat > "$STATE_FILE" << STATE
{
  "environment": "$ENV",
  "deployed_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
  "compose_file": "$COMPOSE_FILE",
  "status": "healthy",
  "health_checks_passed": $HEALTH_PASSES
}
STATE

echo ""
echo "=============================================="
echo "K9: Deployment SUCCESSFUL"
echo "K9: Environment: $ENV"
echo "K9: All health checks passed."
echo "=============================================="
"%,

  # ── Rollback script ────────────────────────────────────────────
  # Restores the previous deployment state. Called automatically
  # on health check failure (if auto_rollback is true), or
  # manually via: just rollback-stack <env>
  rollback = m%"
#!/bin/sh
# SPDX-License-Identifier: PMPL-1.0-or-later
# Rollback for container-deploy-stack
set -eu

ENV="${1:-dev}"
COMPOSE_FILE="selur-compose.${ENV}.toml"
PREV_STATE_FILE="/tmp/k9-deploy-state-${ENV}.prev.json"

echo "K9: Initiating rollback for environment: $ENV"

if [ ! -f "$PREV_STATE_FILE" ]; then
    echo "ERROR: No previous deployment state found."
    echo "K9: Cannot rollback — no known-good state to restore."
    exit 1
fi

echo "K9: Stopping current services..."
selur-compose -f "$COMPOSE_FILE" down

echo "K9: Restoring previous deployment..."
selur-compose -f "$COMPOSE_FILE" up -d

echo "K9: Rollback complete."
echo "K9: Verify with: selur-compose -f $COMPOSE_FILE ps"
"%,

  # ── Teardown script ────────────────────────────────────────────
  # Completely removes all services and volumes for an environment.
  # Use with caution — this destroys data.
  teardown = m%"
#!/bin/sh
# SPDX-License-Identifier: PMPL-1.0-or-later
# Teardown for container-deploy-stack
set -eu

ENV="${1:-dev}"
COMPOSE_FILE="selur-compose.${ENV}.toml"

echo "K9: Tearing down environment: $ENV"
echo "WARNING: This will destroy all data including database volumes."
echo "K9: Press Ctrl+C within 5 seconds to abort..."
sleep 5

selur-compose -f "$COMPOSE_FILE" down --volumes --remove-orphans

echo "K9: Environment $ENV torn down completely."
"%,
} in

# ─────────────────────────────────────────────────────────────────────
# Export
#
# The top-level record is what the k9 runtime and other tools consume.
# It bundles the pedigree (self-description), service definitions,
# environment configs, deployment strategy, signing requirements,
# and executable scripts into a single self-validating unit.
# ─────────────────────────────────────────────────────────────────────
{
  pedigree = component_pedigree,
  services = services,
  environments = environments,
  strategy = strategy,
  signing = signing,
  scripts = scripts,

  # Security check: This component requires Hunt level.
  # The Leash system will verify this before allowing execution.
  required_level = 'Hunt,

  # Usage instructions for operators
  usage = m%"
CONTAINER DEPLOYMENT STACK — Usage Guide
=========================================

This is a Hunt-level k9 component. Before running:

  1. Verify signature:  ./must verify examples/container-deploy.k9.ncl
  2. Authorize:         just authorize examples/container-deploy.k9.ncl

Deploy to an environment:

  just deploy-stack dev         # Development (single replica)
  just deploy-stack staging     # Staging (2 replicas, signed images)
  just deploy-stack production  # Production (3 replicas, full verification)

Rollback:

  just rollback-stack dev

Teardown (destroys data):

  just teardown-stack dev

Generate compose file without deploying:

  just generate-compose dev
"%,
}
